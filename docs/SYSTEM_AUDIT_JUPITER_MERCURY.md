# ğŸ–¥ï¸ Mangwale AI System Audit: Jupiter + Mercury

**Date:** December 18, 2024 (UPDATED)  
**Purpose:** Resource audit, identify gaps, optimize sharing between servers

---

## ğŸ”· CORRECTED Server Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            JUPITER (Brain)                              â”‚
â”‚                         192.168.0.xxx (This Server)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  RAM: 32GB | CPU: Ryzen 5 5500 (6c/12t) | GPU: RTX 3060 12GB           â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  AI Services    â”‚  â”‚  Flow Engine    â”‚  â”‚  Data Layer     â”‚         â”‚
â”‚  â”‚  - vLLM (11GB)  â”‚  â”‚  - YAML Flows   â”‚  â”‚  - PostgreSQL   â”‚         â”‚
â”‚  â”‚    GPU â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚  â”‚  - Executors    â”‚  â”‚  - Redis        â”‚         â”‚
â”‚  â”‚  - NLU (2.6GB)  â”‚  â”‚  - Agents       â”‚  â”‚  - OpenSearch   â”‚         â”‚
â”‚  â”‚  - Search API   â”‚  â”‚                 â”‚  â”‚                 â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                         â”‚
â”‚  GPU: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘] 11.2/12 GB (93% used by vLLM) â”‚
â”‚  RAM: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 19/32 GB (60% used)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            MERCURY (Voice)                              â”‚
â”‚                           192.168.0.151                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  RAM: 16GB | CPU: Ryzen 5 5500 (6c/12t) | GPU: RTX 3060 12GB           â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  Voice Services â”‚  â”‚  Exotel Stack   â”‚  â”‚  GPU (UNDERUSED)â”‚         â”‚
â”‚  â”‚  - ASR (454MB)  â”‚  â”‚  - IVR Service  â”‚  â”‚  - Used: 1.3GB  â”‚         â”‚
â”‚  â”‚  - TTS (870MB)  â”‚  â”‚  - Backend      â”‚  â”‚  - FREE: 10.7GB â”‚         â”‚
â”‚  â”‚  - Orchestrator â”‚  â”‚  - Admin UI     â”‚  â”‚  - 89% IDLE!    â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                         â”‚
â”‚  GPU: [â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 1.3/12 GB (11% used)          â”‚
â”‚  RAM: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 4.6/16 GB (30% used)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        COMBINED RESOURCES                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  TOTAL GPU VRAM:  24 GB (2x RTX 3060)                                  â”‚
â”‚  TOTAL RAM:       48 GB (32 + 16)                                      â”‚
â”‚  TOTAL CPU:       24 threads (12 + 12)                                 â”‚
â”‚  Network:         100 Mbps LAN (0.4ms latency)                         â”‚
â”‚  âš ï¸  NO NVLINK/INFINIBAND - Cannot combine GPUs directly               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š ACTUAL Resource Usage

### Jupiter (Brain Server)
| Resource | Used | Total | % |
|----------|------|-------|---|
| **GPU VRAM** | 11.2 GB | 12 GB | **93%** (vLLM Qwen2.5-7B) |
| **RAM** | 19 GB | 32 GB | 60% |
| **CPU** | Low | 12 threads | ~10% |

| Container | RAM | GPU | Purpose |
|-----------|-----|-----|---------|
| mangwale-ai-vllm | 4.5 GB | **11.2 GB** | Qwen2.5-7B-AWQ (GPU) |
| mangwale-ai-nlu | 2.6 GB | - | IndicBERT NLU |
| search-opensearch | 2.7 GB | - | Product Search |
| Others | ~9 GB | - | PostgreSQL, Redis, etc. |

### Mercury (Voice Server)
| Resource | Used | Total | % |
|----------|------|-------|---|
| **GPU VRAM** | 1.3 GB | 12 GB | **11%** âš ï¸ UNDERUTILIZED |
| **RAM** | 4.6 GB | 16 GB | 30% |
| **CPU** | Low | 12 threads | ~5% |

| Container | RAM | GPU | Purpose |
|-----------|-----|-----|---------|
| mangwale-tts | 933 MB | 870 MB | XTTS Text-to-Speech |
| mangwale-asr | 808 MB | 454 MB | Whisper ASR |
| mangwale-orchestrator | 310 MB | - | Voice Flow |
| exotel-service | 32 MB | - | IVR Telephony |

---

## ğŸ¯ Admin Dashboard Audit (72 Pages)

### âœ… Fully Implemented

| Category | Pages | Status |
|----------|-------|--------|
| **NLU & Training** | /nlu, /intents, /training/*, /nlu-testing | âœ… Complete |
| **Flow Management** | /flows, /flows/editor, /flow-analytics | âœ… Complete |
| **LLM Management** | /llm-providers, /llm-models, /llm-failover, /llm-chat, /llm-cost-tracking | âœ… Complete |
| **Voice** | /voice/xtts, /voice/orpheus | âœ… Complete |
| **Vision** | /vision/* (15 pages: menu-ocr, food-quality, cameras, etc.) | âœ… Complete |
| **Analytics** | /analytics, /search-analytics, /intent-analytics, /llm-analytics | âœ… Complete |
| **Settings** | /settings, /api-keys, /webhooks, /secrets | âœ… Complete |
| **Agents** | /agents, /agent-testing, /agent-settings | âœ… Complete |
| **Infrastructure** | /docker, /monitoring, /vllm-settings | âœ… Complete |

### âš ï¸ Missing Pages Needed

| Feature | Current Status | Priority | Action Required |
|---------|---------------|----------|-----------------|
| **RAG/Documents** | âŒ No UI | HIGH | Create /admin/rag/documents page |
| **User Profiles** | âŒ No UI | HIGH | Create /admin/user-profiles page |
| **User Insights** | âŒ No UI | MEDIUM | Create /admin/user-insights page |
| **Conversation Memory** | âŒ No UI | MEDIUM | Create /admin/conversation-memory page |
| **Mercury Services** | âŒ No UI | LOW | Add Mercury status to /monitoring |

---

## ğŸ‘¤ User Profiling System Audit

### âœ… Backend Implementation (Complete)
```
Database Tables:
â”œâ”€â”€ user_profiles        â†’ Explicit preferences
â”œâ”€â”€ user_insights        â†’ AI-extracted insights
â”œâ”€â”€ user_item_interactions â†’ Item behavior tracking
â”œâ”€â”€ user_search_patterns â†’ Search history
â”œâ”€â”€ conversation_insights â†’ Real-time extraction
â””â”€â”€ conversation_memory  â†’ Session context
```

### Profile Fields Available
| Category | Fields | Source |
|----------|--------|--------|
| **Dietary** | dietary_type, dietary_restrictions[], allergies[], disliked_ingredients[] | Conversation |
| **Food Taste** | favorite_cuisines (jsonb), spice_preference | Conversation, Orders |
| **Shopping** | avg_order_value, order_frequency, price_sensitivity | Order History |
| **Time** | preferred_meal_times (jsonb) | Order Times |
| **Personality** | communication_tone, personality_traits (jsonb) | Conversation Analysis |
| **Completeness** | profile_completeness (0-100%) | Calculated |

### âš ï¸ Issues Found
1. **No Admin UI** - Cannot view/edit user profiles in dashboard
2. **Profile building triggers** - Need to verify when `updateProfileFromConversation()` is called
3. **Insight quality** - Rule-based extraction is basic, LLM analysis available but costly

### ğŸ”§ Recommended Actions
1. Add `/admin/user-profiles` page with search/filter/edit capabilities
2. Add profile insights to conversation view
3. Add profile completeness metrics to analytics

---

## ğŸ“š RAG System Audit

### âœ… Backend Implementation
- **RagContextService** - Retrieves from OpenSearch, formats for LLM
- **OpenSearch indexes** - Products are indexed
- **Search endpoints** - `/search/semantic/food`, `/v2/search/items`

### âš ï¸ Missing Features
| Feature | Status | Action |
|---------|--------|--------|
| Document Upload UI | âŒ Missing | Create upload page |
| Custom Knowledge Base | âŒ Missing | Add document ingestion endpoint |
| FAQ Management | âŒ Missing | Create FAQ CRUD interface |
| Vector Embeddings for Docs | âš ï¸ Only products | Extend to custom documents |

### Current RAG Flow
```
User Query â†’ OpenSearch â†’ Product Results â†’ Format as Context â†’ LLM Prompt
```

### Needed RAG Flow
```
User Query â†’ [OpenSearch Products + Document Vectors + FAQ Base] â†’ Combined Context â†’ LLM
```

---

## ï¿½ GPU COMBINATION STRATEGIES

### âŒ What WON'T Work
| Method | Why Not |
|--------|---------|
| **NVLink** | Requires same-system GPUs + NVLink bridge |
| **SLI** | Not supported for compute, only gaming |
| **Direct GPU Pooling** | GPUs in different machines can't share VRAM |
| **CUDA MPS across network** | Not supported |

### âœ… What WILL Work: Distributed Inference

#### Option 1: Model Parallelism via vLLM Multi-Node (BEST for bigger models)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Run a SINGLE larger model across BOTH GPUs using Ray + vLLM            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Jupiter GPU (12GB)          Mercury GPU (12GB)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚ Model Shard 1   â”‚â—€â”€â”€Rayâ”€â”€â–¶â”‚ Model Shard 2   â”‚                       â”‚
â”‚  â”‚ (Layers 0-15)   â”‚  TCP/IP â”‚ (Layers 16-31)  â”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                                         â”‚
â”‚  COMBINED: ~22GB usable VRAM for model                                 â”‚
â”‚  Can run: Qwen2.5-32B, Llama3-70B-AWQ, Mixtral-8x7B                    â”‚
â”‚                                                                         â”‚
â”‚  âš ï¸ Limitation: 100Mbps network = ~10MB/s = SLOW tensor transfers      â”‚
â”‚  Recommendation: Upgrade to 1Gbps or 10Gbps network first              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation:**
```bash
# On Jupiter (head node)
ray start --head --port=6379

# On Mercury (worker node)  
ray start --address='jupiter-ip:6379'

# Run vLLM with tensor parallelism
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-32B-Instruct-AWQ \
  --tensor-parallel-size 2 \
  --pipeline-parallel-size 1
```

**Pros:** Can run 32B+ models  
**Cons:** Network bottleneck, complex setup, latency increase

---

#### Option 2: Load Balancing (BEST for current network)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Run SAME model on BOTH GPUs, load balance requests                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Jupiter vLLM (Qwen 7B)      Mercury vLLM (Qwen 7B)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚ Full Model      â”‚         â”‚ Full Model      â”‚                       â”‚
â”‚  â”‚ Port: 8002      â”‚         â”‚ Port: 8002      â”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚           â”‚                           â”‚                                 â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                     â–¼                                                   â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚              â”‚ Load Balancer â”‚ (Nginx/HAProxy)                         â”‚
â”‚              â”‚ Round Robin   â”‚                                          â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚                                                                         â”‚
â”‚  Result: 2x throughput, same latency, redundancy                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation:**
```bash
# Deploy vLLM on Mercury (currently empty)
ssh ubuntu@192.168.0.151
docker run -d --gpus all \
  -v /models:/models \
  -p 8002:8002 \
  vllm/vllm-openai:latest \
  --model Qwen/Qwen2.5-7B-Instruct-AWQ \
  --quantization awq \
  --max-model-len 4096 \
  --port 8002

# Update Jupiter with load balancer
# In nginx.conf:
upstream vllm_cluster {
    server localhost:8002;
    server 192.168.0.151:8002;
}
```

**Pros:** Simple, doubles throughput, fault-tolerant  
**Cons:** Same model size limit (7B)

---

#### Option 3: Specialized Model Distribution (RECOMMENDED)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Different specialized models on each GPU                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  JUPITER GPU (12GB)              MERCURY GPU (12GB)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ MAIN LLM            â”‚         â”‚ SPECIALIZED         â”‚               â”‚
â”‚  â”‚ Qwen2.5-7B-AWQ      â”‚         â”‚ + ASR (Whisper) âœ“   â”‚               â”‚
â”‚  â”‚ - General chat      â”‚         â”‚ + TTS (XTTS) âœ“      â”‚               â”‚
â”‚  â”‚ - Intent routing    â”‚         â”‚ + Code model?       â”‚               â”‚
â”‚  â”‚ - Flow decisions    â”‚         â”‚ + Embedding model   â”‚               â”‚
â”‚  â”‚ [11GB used]         â”‚         â”‚ [1.3GB + 6GB free]  â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                                         â”‚
â”‚  Mercury free space: ~6-8GB - can add:                                 â”‚
â”‚  â€¢ DeepSeek-Coder-7B for code generation                               â”‚
â”‚  â€¢ E5-Large embeddings (2GB) for RAG                                   â”‚
â”‚  â€¢ Specialized food/order model                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ RECOMMENDED OPTIMIZATION PLAN

### Phase 1: Quick Wins (No hardware changes)

1. **Add GPU Embeddings on Mercury** (+2GB VRAM)
```bash
# Move embedding service from CPU to Mercury GPU
# Current: search-embedding-service on Jupiter (CPU)
# Target: GPU-accelerated embeddings on Mercury
```

2. **Optimize Jupiter vLLM** (save ~2GB VRAM)
```bash
# Reduce max-model-len from 4096 to 2048 (sufficient for most tasks)
# Current: 11.2GB used
# Target: ~9GB used, freeing space for NLU on GPU
```

### Phase 2: Load Balancing (Medium effort)

3. **Deploy second vLLM on Mercury**
```bash
# Use 6GB of Mercury's free VRAM for second Qwen instance
# Result: 2x LLM throughput
```

4. **Add Nginx load balancer**
```bash
# Balance requests between Jupiter and Mercury vLLM
# Automatic failover if one goes down
```

### Phase 3: Network Upgrade (Future)

5. **Upgrade to 1Gbps or 10Gbps LAN**
```
Current: 100Mbps (~12MB/s max)
Target: 1Gbps (~125MB/s) or 10Gbps (1.25GB/s)

After network upgrade, can do:
- Tensor parallelism across both GPUs
- Run 32B+ models split across both machines
```

---

## ğŸ“Š RESOURCE OPTIMIZATION SUMMARY

| Current | After Phase 1 | After Phase 2 |
|---------|---------------|---------------|
| Jupiter GPU: 93% | Jupiter GPU: 80% | Both GPUs: 70% |
| Mercury GPU: 11% | Mercury GPU: 30% | Both GPUs: 70% |
| LLM Throughput: 1x | LLM Throughput: 1x | LLM Throughput: 2x |
| Embeddings: CPU | Embeddings: GPU | Embeddings: GPU |

### Quick Action Items:
- [ ] **Now:** Add embedding model to Mercury GPU
- [ ] **This week:** Deploy load-balanced vLLM on Mercury
- [ ] **This month:** Upgrade network to 1Gbps
- [ ] **Future:** Implement Ray-based model parallelism for 32B models

---

## ğŸ“‹ Recommended Actions

### HIGH Priority
1. **Move vLLM to Mercury GPU** - Major performance boost, GPU is 89% unused
2. **Create User Profiles Admin Page** - Profiles are built but can't be viewed
3. **Create RAG/Documents Admin Page** - Support knowledge base uploads

### MEDIUM Priority
4. **Add profile insights to conversation view** - Show user context during support
5. **Add Mercury status to monitoring page** - Single pane of glass
6. **Upgrade to Qwen2.5-14B-AWQ** - Better reasoning with available VRAM

### LOW Priority
7. **GPU-accelerated embeddings** - Move embedding service to Mercury
8. **Move NLU to Mercury** - If Jupiter RAM becomes constrained

---

## ğŸ”— SSH Access Configured

```bash
# Passwordless SSH from Jupiter to Mercury
ssh ubuntu@192.168.0.151

# Quick GPU check
ssh ubuntu@192.168.0.151 "nvidia-smi"

# Quick container check
ssh ubuntu@192.168.0.151 "docker ps --format 'table {{.Names}}\t{{.Status}}'"
```

---

## ğŸ“ Notes

- Mercury GPU (RTX 3060) has **10.7GB free VRAM** - significant opportunity
- Jupiter running vLLM on CPU is wasteful when GPU is available
- User profiling backend is solid but needs admin UI
- RAG works for products but needs document upload capability
- No duplication between servers - clean separation currently
