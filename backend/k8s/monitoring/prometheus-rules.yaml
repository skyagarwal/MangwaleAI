# Prometheus Alert Rules for Mangwale
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: mangwale-alerts
  namespace: mangwale
  labels:
    release: prometheus
    app: mangwale
spec:
  groups:
    # Backend API Alerts
    - name: mangwale-backend
      rules:
        - alert: BackendHighLatency
          expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{app="mangwale-backend"}[5m])) by (le)) > 2
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Backend API p95 latency > 2s"
            description: "The p95 latency is {{ $value }}s for the last 5 minutes"

        - alert: BackendHighErrorRate
          expr: sum(rate(http_requests_total{app="mangwale-backend",status=~"5.."}[5m])) / sum(rate(http_requests_total{app="mangwale-backend"}[5m])) > 0.05
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "Backend error rate > 5%"
            description: "Error rate is {{ $value | humanizePercentage }}"

        - alert: BackendPodCrashLooping
          expr: increase(kube_pod_container_status_restarts_total{namespace="mangwale",container="backend"}[1h]) > 3
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Backend pod crash looping"
            description: "Pod {{ $labels.pod }} has restarted {{ $value }} times in the last hour"

        - alert: BackendHighMemory
          expr: container_memory_usage_bytes{namespace="mangwale",container="backend"} / container_spec_memory_limit_bytes{namespace="mangwale",container="backend"} > 0.9
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Backend memory usage > 90%"
            description: "Pod {{ $labels.pod }} memory usage is {{ $value | humanizePercentage }}"

    # vLLM Alerts
    - name: vllm
      rules:
        - alert: VllmHighQueueLength
          expr: vllm_pending_requests > 50
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: "vLLM request queue > 50"
            description: "vLLM has {{ $value }} pending requests"

        - alert: VllmHighLatency
          expr: histogram_quantile(0.95, sum(rate(vllm_request_latency_seconds_bucket[5m])) by (le)) > 30
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "vLLM p95 latency > 30s"
            description: "vLLM inference latency is {{ $value }}s"

        - alert: VllmDown
          expr: up{job="vllm"} == 0
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: "vLLM service is down"
            description: "vLLM is not responding to health checks"

        - alert: VllmGpuMemoryHigh
          expr: nvidia_gpu_memory_used_bytes / nvidia_gpu_memory_total_bytes > 0.95
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "GPU memory usage > 95%"
            description: "GPU {{ $labels.gpu }} memory is {{ $value | humanizePercentage }}"

    # Database Alerts
    - name: database
      rules:
        - alert: PostgresConnectionsHigh
          expr: pg_stat_activity_count / pg_settings_max_connections > 0.8
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "PostgreSQL connections > 80%"
            description: "{{ $value | humanizePercentage }} of max connections used"

        - alert: PostgresReplicationLag
          expr: pg_replication_lag_seconds > 30
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: "PostgreSQL replication lag > 30s"
            description: "Replica lag is {{ $value }}s"

        - alert: PostgresSlowQueries
          expr: rate(pg_stat_statements_mean_time_seconds[5m]) > 5
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "PostgreSQL slow queries detected"
            description: "Average query time is {{ $value }}s"

        - alert: PostgresDown
          expr: pg_up == 0
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: "PostgreSQL is down"
            description: "PostgreSQL is not responding"

    # Redis Alerts
    - name: redis
      rules:
        - alert: RedisMemoryHigh
          expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.9
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Redis memory > 90%"
            description: "Redis memory usage is {{ $value | humanizePercentage }}"

        - alert: RedisDown
          expr: redis_up == 0
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: "Redis is down"
            description: "Redis is not responding"

        - alert: RedisReplicationBroken
          expr: redis_connected_slaves < 2
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Redis replication broken"
            description: "Only {{ $value }} slaves connected (expected 2)"

    # OpenSearch Alerts
    - name: opensearch
      rules:
        - alert: OpenSearchClusterRed
          expr: opensearch_cluster_health_status{color="red"} == 1
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: "OpenSearch cluster is RED"
            description: "OpenSearch cluster health is critical"

        - alert: OpenSearchClusterYellow
          expr: opensearch_cluster_health_status{color="yellow"} == 1
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "OpenSearch cluster is YELLOW"
            description: "OpenSearch cluster has unassigned shards"

        - alert: OpenSearchDiskHigh
          expr: opensearch_fs_data_free_bytes / opensearch_fs_data_total_bytes < 0.15
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "OpenSearch disk < 15% free"
            description: "Only {{ $value | humanizePercentage }} disk space remaining"

    # WhatsApp Channel Alerts
    - name: whatsapp
      rules:
        - alert: WhatsAppWebhookErrors
          expr: sum(rate(whatsapp_webhook_errors_total[5m])) > 1
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "WhatsApp webhook errors detected"
            description: "{{ $value }} webhook errors per second"

        - alert: WhatsAppMessageDeliveryFailed
          expr: sum(rate(whatsapp_message_delivery_failed_total[10m])) / sum(rate(whatsapp_message_sent_total[10m])) > 0.05
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "WhatsApp delivery failure rate > 5%"
            description: "{{ $value | humanizePercentage }} of messages failing"

    # Business Metrics Alerts
    - name: business
      rules:
        - alert: LowConversationVolume
          expr: sum(increase(conversations_started_total[1h])) < 10
          for: 2h
          labels:
            severity: info
          annotations:
            summary: "Low conversation volume"
            description: "Only {{ $value }} conversations in the last hour"

        - alert: HighAbandonmentRate
          expr: sum(rate(conversations_abandoned_total[1h])) / sum(rate(conversations_started_total[1h])) > 0.3
          for: 1h
          labels:
            severity: warning
          annotations:
            summary: "High conversation abandonment rate"
            description: "{{ $value | humanizePercentage }} of conversations abandoned"

        - alert: LlmCostAnomaly
          expr: sum(increase(llm_tokens_used_total[1h])) > sum(increase(llm_tokens_used_total[1h] offset 24h)) * 2
          for: 30m
          labels:
            severity: warning
          annotations:
            summary: "LLM token usage anomaly"
            description: "Token usage is 2x higher than same time yesterday"

    # ═══════════════════════════════════════════════════════════════
    # MESSAGE GATEWAY ALERTS (Phase 3 Architecture)
    # ═══════════════════════════════════════════════════════════════
    - name: message-gateway
      rules:
        - alert: MessageGatewayHighLatency
          expr: histogram_quantile(0.95, sum(rate(mangwale_message_processing_duration_seconds_bucket[5m])) by (le)) > 5
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Message Gateway p95 latency > 5s"
            description: "Message processing latency is {{ $value }}s"

        - alert: MessageGatewayHighDeduplication
          expr: sum(rate(mangwale_messages_deduplicated_total[5m])) / sum(rate(mangwale_messages_received_total[5m])) > 0.1
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "High deduplication rate > 10%"
            description: "{{ $value | humanizePercentage }} of messages are duplicates - possible client issue"

        - alert: MessageGatewayNoMessages
          expr: sum(rate(mangwale_messages_received_total[10m])) == 0
          for: 15m
          labels:
            severity: critical
          annotations:
            summary: "No messages received in 15 minutes"
            description: "Message Gateway is not receiving any messages"

    # ═══════════════════════════════════════════════════════════════
    # CONTEXT ROUTER ALERTS (Phase 3 Architecture)
    # ═══════════════════════════════════════════════════════════════
    - name: context-router
      rules:
        - alert: RoutingDecisionSlowdown
          expr: histogram_quantile(0.95, sum(rate(mangwale_routing_decision_duration_seconds_bucket[5m])) by (le)) > 0.1
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Routing decisions taking > 100ms (p95)"
            description: "Context router is slow: {{ $value }}s"

        - alert: HighErrorRouteRate
          expr: sum(rate(mangwale_routes_taken_total{route="error"}[5m])) / sum(rate(mangwale_routes_taken_total[5m])) > 0.1
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High error route rate > 10%"
            description: "{{ $value | humanizePercentage }} of messages routing to error handler"

    # ═══════════════════════════════════════════════════════════════
    # NLU SERVICE ALERTS (Phase 3 Architecture)
    # ═══════════════════════════════════════════════════════════════
    - name: nlu-service
      rules:
        - alert: NLUHighLatency
          expr: histogram_quantile(0.95, sum(rate(mangwale_nlu_processing_duration_seconds_bucket[5m])) by (le)) > 1
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "NLU processing p95 latency > 1s"
            description: "NLU inference is slow: {{ $value }}s"

        - alert: NLULowConfidence
          expr: histogram_quantile(0.50, sum(rate(mangwale_intent_confidence_bucket[5m])) by (le)) < 0.5
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "NLU median confidence < 50%"
            description: "NLU model may need retraining - median confidence is {{ $value }}"

        - alert: NLUUnknownIntentSpike
          expr: sum(rate(mangwale_intent_classifications_total{intent="unknown"}[5m])) / sum(rate(mangwale_intent_classifications_total[5m])) > 0.3
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "High unknown intent rate > 30%"
            description: "{{ $value | humanizePercentage }} of intents are unknown - check for new user patterns"

    # ═══════════════════════════════════════════════════════════════
    # FLOW ENGINE ALERTS (Phase 3 Architecture)
    # ═══════════════════════════════════════════════════════════════
    - name: flow-engine
      rules:
        - alert: FlowEngineHighFailureRate
          expr: sum(rate(mangwale_flow_completions_total{status="failed"}[5m])) / sum(rate(mangwale_flow_completions_total[5m])) > 0.1
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Flow completion failure rate > 10%"
            description: "{{ $value | humanizePercentage }} of flows are failing"

        - alert: FlowEngineLongRunning
          expr: histogram_quantile(0.95, sum(rate(mangwale_flow_duration_seconds_bucket[5m])) by (le)) > 120
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Flows taking > 2 minutes (p95)"
            description: "Long-running flows detected: {{ $value }}s"

        - alert: FlowEngineHighCancellationRate
          expr: sum(rate(mangwale_flow_completions_total{status="cancelled"}[1h])) / sum(rate(mangwale_flow_completions_total[1h])) > 0.2
          for: 30m
          labels:
            severity: info
          annotations:
            summary: "High flow cancellation rate > 20%"
            description: "{{ $value | humanizePercentage }} of flows are being cancelled - review UX"

    # ═══════════════════════════════════════════════════════════════
    # CHANNEL ALERTS (Phase 3 Architecture)
    # ═══════════════════════════════════════════════════════════════
    - name: channel-health
      rules:
        - alert: ChannelHighErrorRate
          expr: sum by (channel) (rate(mangwale_channel_errors_total[5m])) / sum by (channel) (rate(mangwale_channel_messages_total[5m])) > 0.05
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Channel {{ $labels.channel }} error rate > 5%"
            description: "{{ $value | humanizePercentage }} error rate on {{ $labels.channel }}"

        - alert: ChannelHighLatency
          expr: histogram_quantile(0.95, sum by (channel, le) (rate(mangwale_channel_latency_seconds_bucket[5m]))) > 10
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Channel {{ $labels.channel }} p95 latency > 10s"
            description: "High latency on {{ $labels.channel }}: {{ $value }}s"

        - alert: ChannelDown
          expr: sum by (channel) (rate(mangwale_channel_messages_total[10m])) == 0 and sum by (channel) (rate(mangwale_channel_messages_total[10m] offset 1h)) > 0
          for: 15m
          labels:
            severity: critical
          annotations:
            summary: "Channel {{ $labels.channel }} appears down"
            description: "No messages on {{ $labels.channel }} for 15 min but was active 1 hour ago"
